import numpy as np
from collections import Counter
import re
import string

def sparse_cross_entropy(z, y):
    return -np.log(z[0, y])

def sparse_cross_entropy_batch(z, y):
    return -np.log(z[np.arange(len(y)), y] + 1e-9)
    # return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))

def clean_text(text: str) -> str:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞.
    """
    # –®–∞–≥ 1: –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    text = text.lower()

    # –®–∞–≥ 2: –£–¥–∞–ª—è–µ–º URL-–∞–¥—Ä–µ—Å–∞
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # –®–∞–≥ 3: –£–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏ (—É–ª—É—á—à–µ–Ω–Ω–æ–µ –∏ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ)
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F700-\U0001F77F"  # alchemical symbols
        "\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
        "\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
        "\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs (–≤–∫–ª—é—á–∞—è ü§ù)
        "\U0001FA00-\U0001FA6F"  # Chess Symbols
        "\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
        "\U00002702-\U000027B0"  # Dingbats
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    # –®–∞–≥ 4: –£–¥–∞–ª—è–µ–º —Ü–∏—Ñ—Ä—ã
    text = re.sub(r'[0-9]', '', text)

    # –®–∞–≥ 5: –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –≤–∫–ª—é—á–∞—è –¥–µ—Ñ–∏—Å—ã
    text = re.sub(r'[¬´¬ª‚Äû‚Äú"‚Äù"‚Äò‚Äô`‚Äõ‚Äî‚Äì-]', ' ', text)  # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª, —á—Ç–æ–±—ã –Ω–µ —Å–∫–ª–µ–∏–≤–∞—Ç—å —Å–ª–æ–≤–∞

    # –®–∞–≥ 6: –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞, –∏—Å–∫–ª—é—á–∞—è –¥–µ—Ñ–∏—Å, —Ç.–∫. —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ –µ–≥–æ
    # –≠—Ç–æ —Ç–∞–∫–∂–µ —É–¥–∞–ª–∏—Ç —Å–∏–º–≤–æ–ª—ã –≤—Ä–æ–¥–µ `_`, `+`, `=`, –∏ —Ç.–¥.
    punct_to_remove = string.punctuation.replace('-', '')
    table = str.maketrans('', '', punct_to_remove)
    text = text.translate(table)

    # –®–∞–≥ 7: –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = " ".join(text.split())

    return text

def softmax(x):
    # print(x)
    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return e_x / e_x.sum(axis=1, keepdims=True)


def relu(x):
    return np.maximum(0, x)


def relu_deriv(x):
    return np.where(x > 0, 1, 0)


def get_unique_words(min_freq, couples_arr):
    joined_couples = (" ".join(couples_arr).lower()).split()

    word_counts = Counter(joined_couples)

    unique_words = [word for word, count in word_counts.items() if count > min_freq and len(word) < 25]

    unique_words.append('<unk>')
    unique_words.append('<eos>')

    print(f"–ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(word_counts)}")
    print(f"–ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (—Å–ª–æ–≤–∞ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è > {min_freq} —Ä–∞–∑): {len(unique_words)}")

    return unique_words

def parse_file_data_by_lines(data_path):
    raw_data = []
    with open(data_path, "r", encoding="utf-8") as file:
        for line in file:
            raw_data.append(line.replace("\n", ""))
    return raw_data

def parse_file_data_by_dots(data_path):
    raw_data = []
    with open(data_path, "r", encoding="utf-8") as file:
        text = file.read()  # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ä–∞–∑—É
        sentences = re.split(r'[-.?!‚Äì\n:;]', text)

        # print(sentences)
        for couple in sentences:
            # print(couple)
            if len(couple.split()) < 2: continue
            raw_data.append(couple.strip().lower().replace("\n", ""))
    return raw_data